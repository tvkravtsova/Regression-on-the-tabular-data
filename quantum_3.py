# -*- coding: utf-8 -*-
"""Quantum_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxPWOE3u3YWjnUpQOqV4y7BmGsMj-T6x
"""

!pip install tensorflow

#importing libraries
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the datasets
df = pd.read_csv('/content/internship_train.csv')
df_test= pd.read_csv("/content/internship_hidden_test.csv")
df.shape, df_test.shape

df.sample(10)

#dataset exploration
df.info()

"""Our data has no missing values.

"""

df_test.sample(10)

df_test.info()

df.duplicated().sum()

df.describe()

#distribution of some features
header = ['0','6','7','9','30','44','51','52']
df.hist(column=header,figsize=(15,15), bins=70)
plt.subplots_adjust(wspace = 0.5, hspace = 0.5)
plt.show()

"""As we see from the results of describe() function and visualization of distribution, most of our features has uniform distribution."""

corr_matrix = df.corr(method='spearman')

print("Spearman's rank correlation coefficient matrix: \n", corr_matrix)

"""As we see Spearman's rank correlation coefficient is close to zero, indicating a weak or no correlation between the two variables. Uniform distribution  means that the values are evenly distributed across the range of possible values. So it is not a meaningful measure of the relationship between the variables in our case.


"""

# LinearRegression
from sklearn.linear_model import LinearRegression


# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)

# Train linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Evaluate model using RMSE metric
y_pred = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

# Random Forest Regressor model
from sklearn.ensemble import RandomForestRegressor

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)

# Train random forest regression model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Evaluate model using RMSE metric
y_pred = rf.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

"""The difference in RMSE values between a linear regression model and a Random Forest Regressor model can be attributed to the ability of
Random Forest Regressors to capture non-linear relationships, interactions between features. 
Linear regression models also assume that the predictor variables are independent of each other.
"""

# Gradient Boosting Regressor model
from sklearn.ensemble import GradientBoostingRegressor

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)

# Train gradient boosting regression model
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gbr.fit(X_train, y_train)

# Evaluate model using RMSE metric
y_pred = gbr.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

# Neural network model

import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import mean_squared_error


# Separate the features and target
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the neural network model
model = keras.Sequential([
    keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)

# Prediction for test dataset using neural network model
y_pred_test = model.predict(df_test)
y_pred_test

# create a new column in the test dataframe with the predicted values
df_test['predicted'] = y_pred_test

df_test.to_csv('test_results.csv', index=False)